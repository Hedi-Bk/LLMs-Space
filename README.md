# LLMs-Space

<p align="center" width="300">Welcome to my LLMs project collection!  </p>
<p align="center">
  <img src="https://user-images.githubusercontent.com/74038190/216122041-518ac897-8d92-4c6b-9b3f-ca01dcaf38ee.png" width="200"/>
</p>

This space documents my journey toward becoming a LLM Engineer by building real, production-ready systems.
Showcasing my projects in Large Language Models: Fine-Tuning, Optimization & Applied LLM Techniques

## **Featured Projects**

### **1. Fine-Tuning LLaMA 2–7B with QLoRA (1M Samples)**

**Link:** *Coming soon*

Fine-tuning of the LLaMA 2–7B model using the **QLoRA** method:

- Base model quantized in **NF4**
- Full-precision **LoRA adapters**
- **Supervised Fine-Tuning (SFT)** on a 1M curated dataset
- Achieved **training loss = 1.36**
- Runtime: **1634s**, Steps/s: **0.153**, FLOPs: **8.7e15**

| Metric            | Value           |
| ----------------- | --------------- |
| **Training Loss** | **1.36**        |
| **Global Steps**  | **250**         |
| **Runtime**       | **1634 s**      |
| **Samples / sec** | **0.612**       |
| **Steps / sec**   | **0.153**       |
| **Total FLOPs**   | **8.75 × 10¹⁵** |
